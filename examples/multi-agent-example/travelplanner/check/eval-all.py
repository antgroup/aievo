#!/usr/bin/env python3
"""
é›†æˆè¯„ä¼°è„šæœ¬ - åˆå¹¶å››ä¸ªè¯„ä¼°æ­¥éª¤ä¸ºä¸€ä¸ªæ–‡ä»¶
åŒ…å«ï¼šparsing, element_extraction, combination, evaluation
"""

import os
import sys
import json
import argparse
import time
from tqdm import tqdm
from datetime import datetime
from typing import List, Dict, Any
import traceback

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥å¿…è¦æ¨¡å—
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../postprocess")))
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../evaluation")))

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
try:
    from datasets import load_dataset
    from postprocess.openai_request import build_plan_format_conversion_prompt, prompt_chatgpt
    from evaluation.commonsense_constraint import evaluation as commonsense_eval
    from evaluation.hard_constraint import evaluation as hard_eval
except ImportError as e:
    print(f"Warning: Import error: {e}")
    print("Some functionality may not be available")

class IntegratedEvaluator:
    """é›†æˆè¯„ä¼°å™¨ - æ‰§è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹"""
    
    def __init__(self, args):
        self.args = args
        self.query_data_list = None
        self.parsed_results = []
        self.extracted_results = []
        self.submission_list = []
        
        # æµ‹è¯•æ¨¡å¼é™åˆ¶
        if args.test_mode:
            self.limit = 5
            print("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰5ä¸ªè®¡åˆ’")
        elif args.limit:
            self.limit = args.limit
            print(f"ğŸ”¢ é™åˆ¶æ¨¡å¼ï¼šåªå¤„ç†å‰{self.limit}ä¸ªè®¡åˆ’")
        else:
            self.limit = None
        
        # è®¾ç½®è·¯å¾„
        self.setup_paths()
        
    def setup_paths(self):
        """è®¾ç½®æ‰€æœ‰å¿…è¦çš„è·¯å¾„"""
        # åŸºç¡€è·¯å¾„
        self.output_dir = self.args.output_dir
        self.dataset_dir = f'{self.output_dir}/datasets/{self.args.set_type}'
        self.dataset_file = os.path.join(self.dataset_dir, f'travelplanner_{self.args.set_type}_dataset.json')
        
        # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
        self.generation_dir = f'{self.output_dir}/{self.args.set_type}'
        
        # ä¸´æ—¶æ–‡ä»¶è·¯å¾„
        self.tmp_dir = self.args.tmp_dir or f'{self.output_dir}/parse'
        os.makedirs(self.tmp_dir, exist_ok=True)
        
        # æäº¤æ–‡ä»¶è·¯å¾„
        self.submission_dir = self.args.submission_file_dir or f'{self.output_dir}/eval'
        os.makedirs(self.submission_dir, exist_ok=True)
        
        # ç»“æœæ–‡ä»¶è·¯å¾„
        if self.args.mode == 'two-stage':
            suffix = ''
        elif self.args.mode == 'sole-planning':
            suffix = f'_{self.args.strategy}'
        
        self.submission_file = f'{self.submission_dir}/{self.args.set_type}_{self.args.model_name}{suffix}_{self.args.mode}_submission.jsonl'
        
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"æ•°æ®é›†æ–‡ä»¶: {self.dataset_file}")
        print(f"ç”Ÿæˆè®¡åˆ’ç›®å½•: {self.generation_dir}")
        print(f"æœ€ç»ˆæäº¤æ–‡ä»¶: {self.submission_file}")
        
    def load_dataset(self):
        """åŠ è½½æ•°æ®é›†"""
        print("ğŸ“ åŠ è½½æ•°æ®é›†...")
        
        # æ£€æŸ¥æœ¬åœ°æ•°æ®é›†æ–‡ä»¶
        if os.path.exists(self.dataset_file):
            print(f"ä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ•°æ®é›†: {self.dataset_file}")
            try:
                with open(self.dataset_file, 'r', encoding='utf-8') as f:
                    self.query_data_list = json.load(f)
                print(f"æˆåŠŸåŠ è½½ {len(self.query_data_list)} ä¸ªæ•°æ®é¡¹")
                
                # åº”ç”¨é™åˆ¶
                if self.limit:
                    original_length = len(self.query_data_list)
                    self.query_data_list = self.query_data_list[:self.limit]
                    print(f"åº”ç”¨é™åˆ¶ï¼šä» {original_length} ä¸ªå‡å°‘åˆ° {len(self.query_data_list)} ä¸ª")
                
                return
            except Exception as e:
                print(f"åŠ è½½æœ¬åœ°æ•°æ®é›†å¤±è´¥: {e}")
                print("å›é€€åˆ°ä» HuggingFace ä¸‹è½½...")
        
        # ä» HuggingFace ä¸‹è½½
        print(f"ä» HuggingFace ä¸‹è½½æ•°æ®é›†: osunlp/TravelPlanner [{self.args.set_type}]")
        try:
            if self.args.set_type == 'validation':
                hf_dataset = load_dataset('osunlp/TravelPlanner', 'validation')['validation']
            elif self.args.set_type == 'test':
                hf_dataset = load_dataset('osunlp/TravelPlanner', 'test')['test']
            elif self.args.set_type == 'train':
                hf_dataset = load_dataset('osunlp/TravelPlanner', 'train')['train']
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†ç±»å‹: {self.args.set_type}")
            
            # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
            self.query_data_list = [dict(item) for item in hf_dataset]
            print(f"ä» HuggingFace ä¸‹è½½ {len(self.query_data_list)} ä¸ªæ•°æ®é¡¹")
            
            # åº”ç”¨é™åˆ¶
            if self.limit:
                original_length = len(self.query_data_list)
                self.query_data_list = self.query_data_list[:self.limit]
                print(f"åº”ç”¨é™åˆ¶ï¼šä» {original_length} ä¸ªå‡å°‘åˆ° {len(self.query_data_list)} ä¸ª")
            
            # ä¿å­˜åˆ°æœ¬åœ°ä»¥å¤‡åç”¨
            os.makedirs(self.dataset_dir, exist_ok=True)
            with open(self.dataset_file, 'w', encoding='utf-8') as f:
                json.dump(self.query_data_list, f, indent=2, ensure_ascii=False)
            print(f"æ•°æ®é›†å·²ä¿å­˜åˆ°æœ¬åœ°: {self.dataset_file}")
            
        except Exception as e:
            print(f"ä» HuggingFace ä¸‹è½½æ•°æ®é›†å¤±è´¥: {e}")
            raise
    
    def step1_parsing(self):
        """æ­¥éª¤1: è§£æç”Ÿæˆçš„è®¡åˆ’æ–‡æœ¬"""
        print("\nğŸ”„ æ­¥éª¤ 1/4: è§£æè®¡åˆ’æ–‡æœ¬...")
        
        try:
            # æ„å»ºè§£ææç¤ºè¯
            prompt_list = build_plan_format_conversion_prompt(
                directory=self.output_dir,
                set_type=self.args.set_type,
                model_name=self.args.model_name,
                strategy=self.args.strategy,
                mode=self.args.mode
            )
            
            # å‡†å¤‡è¾“å‡ºæ–‡ä»¶
            if self.args.mode == 'two-stage':
                suffix = ''
            elif self.args.mode == 'sole-planning':
                suffix = f'_{self.args.strategy}'
            
            tmp_output_file = f'{self.tmp_dir}/{self.args.set_type}_{self.args.model_name}{suffix}_{self.args.mode}.txt'
            
            total_price = 0
            self.parsed_results = []
            
            print(f"å¤„ç† {len(prompt_list)} ä¸ªè®¡åˆ’...")
            for idx, prompt in enumerate(tqdm(prompt_list)):
                if prompt == "":
                    result = str(idx)
                    self.parsed_results.append(result)
                    with open(tmp_output_file, 'a+', encoding='utf-8') as f:
                        f.write(result + '\n')
                    continue
                
                try:
                    # è°ƒç”¨ LLM è¿›è¡Œè§£æ
                    env_model_name = "Qwen2.5-72B-Instruct"
                    results, _, price = prompt_chatgpt(
                        "You are a helpful assistant.", 
                        index=idx, 
                        save_path=tmp_output_file,
                        user_input=prompt, 
                        model_name=env_model_name, 
                        temperature=0
                    )
                    total_price += price
                    self.parsed_results.append(results)
                    
                except Exception as e:
                    print(f"è§£æç¬¬ {idx} ä¸ªè®¡åˆ’æ—¶å‡ºé”™: {e}")
                    error_result = f"ERROR: {str(e)}"
                    self.parsed_results.append(error_result)
                    with open(tmp_output_file, 'a+', encoding='utf-8') as f:
                        f.write(f"{idx}\t{error_result}\n")
            
            print(f"è§£æå®Œæˆï¼Œæ€»è´¹ç”¨: ${total_price}")
            return True
            
        except Exception as e:
            print(f"è§£ææ­¥éª¤å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def step2_element_extraction(self):
        """æ­¥éª¤2: æå–ç»“æ„åŒ–æ•°æ®"""
        print("\nğŸ¯ æ­¥éª¤ 2/4: æå–ç»“æ„åŒ–æ•°æ®...")
        
        try:
            # è¯»å–è§£æç»“æœ
            if self.args.mode == 'two-stage':
                suffix = ''
            elif self.args.mode == 'sole-planning':
                suffix = f'_{self.args.strategy}'
            
            tmp_file = f'{self.tmp_dir}/{self.args.set_type}_{self.args.model_name}{suffix}_{self.args.mode}.txt'
            
            if not os.path.exists(tmp_file):
                print(f"ä¸´æ—¶æ–‡ä»¶ä¸å­˜åœ¨: {tmp_file}")
                return False
            
            with open(tmp_file, 'r', encoding='utf-8') as f:
                results = f.read().strip().split('\n')
            
            idx_number_list = [i for i in range(1, len(self.query_data_list) + 1)]
            
            print(f"å¤„ç† {len(idx_number_list)} ä¸ªæå–ä»»åŠ¡...")
            success_count = 0
            
            for idx in tqdm(idx_number_list):
                try:
                    # åŠ è½½ç”Ÿæˆçš„è®¡åˆ’
                    plan_file = f'{self.generation_dir}/generated_plan_{idx}.json'
                    if not os.path.exists(plan_file):
                        print(f"è®¡åˆ’æ–‡ä»¶ä¸å­˜åœ¨: {plan_file}")
                        continue
                    
                    with open(plan_file, 'r', encoding='utf-8') as f:
                        generated_plan = json.load(f)
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆç»“æœ
                    plan_key = f'{self.args.model_name}{suffix}_{self.args.mode}_results'
                    if generated_plan[-1][plan_key] not in ["", "Max Token Length Exceeded."]:
                        try:
                            # æå– JSON å†…å®¹
                            result_text = results[idx-1]
                            if '```json' in result_text:
                                result = result_text.split('```json')[1].split('```')[0]
                            else:
                                result = result_text
                            
                            # è§£æ JSON
                            parsed_result = eval(result.strip())
                            
                            # ä¿å­˜è§£æç»“æœ
                            parsed_key = f'{self.args.model_name}{suffix}_{self.args.mode}_parsed_results'
                            generated_plan[-1][parsed_key] = parsed_result
                            
                            success_count += 1
                            
                        except Exception as e:
                            print(f"è§£æç¬¬ {idx} ä¸ªç»“æœæ—¶å‡ºé”™: {e}")
                            # è®¾ç½®ä¸º None è¡¨ç¤ºè§£æå¤±è´¥
                            parsed_key = f'{self.args.model_name}{suffix}_{self.args.mode}_parsed_results'
                            generated_plan[-1][parsed_key] = None
                    else:
                        # æ²¡æœ‰æœ‰æ•ˆç»“æœ
                        parsed_key = f'{self.args.model_name}{suffix}_{self.args.mode}_parsed_results'
                        generated_plan[-1][parsed_key] = None
                    
                    # ä¿å­˜æ›´æ–°çš„è®¡åˆ’æ–‡ä»¶
                    with open(plan_file, 'w', encoding='utf-8') as f:
                        json.dump(generated_plan, f, indent=2, ensure_ascii=False)
                        
                except Exception as e:
                    print(f"å¤„ç†ç¬¬ {idx} ä¸ªæ–‡ä»¶æ—¶å‡ºé”™: {e}")
                    continue
            
            print(f"æå–å®Œæˆï¼ŒæˆåŠŸå¤„ç†: {success_count}/{len(idx_number_list)}")
            return True
            
        except Exception as e:
            print(f"æå–æ­¥éª¤å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def step3_combination(self):
        """æ­¥éª¤3: åˆå¹¶ä¸ºè¯„ä¼°æ ¼å¼"""
        print("\nğŸ“Š æ­¥éª¤ 3/4: åˆå¹¶æ–‡ä»¶...")
        
        try:
            if self.args.mode == 'two-stage':
                suffix = ''
            elif self.args.mode == 'sole-planning':
                suffix = f'_{self.args.strategy}'
            
            idx_number_list = [i for i in range(1, len(self.query_data_list) + 1)]
            self.submission_list = []
            
            print(f"åˆå¹¶ {len(idx_number_list)} ä¸ªè®¡åˆ’...")
            for idx in tqdm(idx_number_list):
                try:
                    plan_file = f'{self.generation_dir}/generated_plan_{idx}.json'
                    if not os.path.exists(plan_file):
                        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºç©ºè®¡åˆ’
                        plan = None
                    else:
                        with open(plan_file, 'r', encoding='utf-8') as f:
                            generated_plan = json.load(f)
                        
                        parsed_key = f'{self.args.model_name}{suffix}_{self.args.mode}_parsed_results'
                        plan = generated_plan[-1].get(parsed_key, None)
                    
                    # æ·»åŠ åˆ°æäº¤åˆ—è¡¨
                    submission_item = {
                        "idx": idx,
                        "query": self.query_data_list[idx-1]['query'],
                        "plan": plan
                    }
                    self.submission_list.append(submission_item)
                    
                except Exception as e:
                    print(f"å¤„ç†ç¬¬ {idx} ä¸ªè®¡åˆ’æ—¶å‡ºé”™: {e}")
                    # æ·»åŠ ç©ºè®¡åˆ’
                    submission_item = {
                        "idx": idx,
                        "query": self.query_data_list[idx-1]['query'],
                        "plan": None
                    }
                    self.submission_list.append(submission_item)
            
            # å†™å…¥æäº¤æ–‡ä»¶ï¼ˆç¬¬ä¸€æ¬¡å†™å…¥æ“ä½œï¼‰
            print(f"ğŸ’¾ å†™å…¥æäº¤æ–‡ä»¶: {self.submission_file}")
            with open(self.submission_file, 'w', encoding='utf-8') as w:
                for unit in self.submission_list:
                    output = json.dumps(unit, ensure_ascii=False)
                    w.write(output + "\n")
            
            print(f"åˆå¹¶å®Œæˆï¼Œç”Ÿæˆ {len(self.submission_list)} ä¸ªæäº¤é¡¹")
            return True
            
        except Exception as e:
            print(f"åˆå¹¶æ­¥éª¤å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def step4_evaluation(self):
        """æ­¥éª¤4: è¯„ä¼°ç»“æœ"""
        print("\nğŸ“ˆ æ­¥éª¤ 4/4: è¯„ä¼°ç»“æœ...")
        
        try:
            # ä½¿ç”¨ç°æœ‰çš„è¯„ä¼°å‡½æ•°
            scores, detailed_scores = self.eval_score(self.args.set_type, self.submission_file)
            
            # è¾“å‡ºç»“æœ
            print("\n" + "="*60)
            print("ğŸ¯ è¯„ä¼°ç»“æœ:")
            print("="*60)
            
            for key in scores:
                percentage = float(int(scores[key]*10000))/100
                print(f"{key}: {percentage}%")
            
            print("\n" + "="*60)
            print("ğŸ“Š è¯¦ç»†ç»“æœ:")
            print("="*60)
            print(json.dumps(detailed_scores, indent=2, ensure_ascii=False))
            
            # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶ï¼ˆç¬¬äºŒæ¬¡å†™å…¥æ“ä½œï¼‰
            self.save_results(scores, detailed_scores)
            
            return True, scores, detailed_scores
            
        except Exception as e:
            print(f"è¯„ä¼°æ­¥éª¤å¤±è´¥: {e}")
            traceback.print_exc()
            return False, None, None
    
    def eval_score(self, set_type: str, file_path: str):
        """è¯„ä¼°åˆ†æ•° - ä»åŸ eval.py ç§»æ¤çš„é€»è¾‘"""
        
        # åŠ è½½æµ‹è¯•è®¡åˆ’
        tested_plans = self.load_line_json_data(file_path)
        
        # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
        hardConstraint_statistic = {level: {day: [] for day in [3,5,7]} for level in ['easy','medium','hard']}
        commonsenseConstraint_statistic = {level: {day: [] for day in [3,5,7]} for level in ['easy','medium','hard']}
        
        delivery_cnt = 0
        plan_constraint_store = []
        
        print(f"è¯„ä¼° {min(len(self.query_data_list), len(tested_plans))} ä¸ªè®¡åˆ’...")
        
        for idx in tqdm(range(0, min(len(self.query_data_list), len(tested_plans)))):
            query_data = self.query_data_list[idx]
            tested_plan = tested_plans[idx]
            
            # æ•°æ®ç±»å‹è½¬æ¢
            if type(query_data) == str:
                query_data = eval(query_data)
            if type(tested_plan) == str:
                tested_plan = eval(tested_plan)
            if type(query_data['local_constraint']) == str:
                query_data['local_constraint'] = eval(query_data['local_constraint'])
            
            # å¸¸è¯†çº¦æŸè¯„ä¼°
            if tested_plan['plan']:
                delivery_cnt += 1
                commonsense_info_box = commonsense_eval(query_data, tested_plan['plan'])
            else:
                commonsense_info_box = None
            
            # ç¡¬çº¦æŸè¯„ä¼°ï¼ˆåªæœ‰é€šè¿‡å¸¸è¯†çº¦æŸæ‰è¯„ä¼°ï¼‰
            if commonsense_info_box and commonsense_info_box['is_not_absent'][0] and commonsense_info_box['is_valid_information_in_sandbox'][0]:
                hard_info_box = hard_eval(query_data, tested_plan['plan'])
            else:
                hard_info_box = None
            
            plan_constraint_store.append({
                'commonsense_constraint': commonsense_info_box,
                'hard_constraint': hard_info_box
            })
            
            commonsenseConstraint_statistic[query_data['level']][query_data['days']].append(commonsense_info_box)
            hardConstraint_statistic[query_data['level']][query_data['days']].append(hard_info_box)
        
        # å¤„ç†çº¦æŸç»Ÿè®¡
        constraint_record = {key: {day: {'house rule':0, 'cuisine':0, 'room type':0, 'transportation':0} for day in [3,5,7]} for key in ['medium','hard']}
        constraint_mapping = {'house rule':'valid_room_rule','cuisine':'valid_cuisine','room type':'valid_room_type','transportation':'valid_transportation'}
        mapping_constraint_record = {key: {day: {'valid_room_rule':0, 'valid_cuisine':0, 'valid_room_type':0, 'valid_transportation':0} for day in [3,5,7]} for key in ['medium','hard']}
        count_record = {key:{day:0 for day in [3,5,7]} for key in ['easy','medium','hard']}
        
        for unit in self.query_data_list:
            if type(unit) == str:
                unit = eval(unit)
            if type(unit['local_constraint']) == str:
                unit['local_constraint'] = eval(unit['local_constraint'])
                
            count_record[unit['level']][unit['days']] += 1
            for key in constraint_record['medium'][3]:
                try:
                    if unit['local_constraint'][key] != None:
                        if unit['level'] in constraint_record:  # åªå¤„ç† medium å’Œ hard çº§åˆ«
                            constraint_record[unit['level']][unit['days']][key] += 1
                            mapping_constraint_record[unit['level']][unit['days']][constraint_mapping[key]] += 1
                except Exception:
                    continue
        
        # ç»Ÿè®¡å¤„ç†
        commonsenseConstraint_statistic_processed = self.statistics(commonsenseConstraint_statistic)
        hardConstraint_statistic_processed = self.statistics(hardConstraint_statistic)
        
        # è®¡ç®—æœ€ç»ˆåˆ†æ•°
        final_all_cnt = 0
        final_commonsense_cnt = 0
        final_hardConstraint_cnt = 0
        
        constraint_dis_record = {"commonsense":{"pass":0,"total":0},"hard":{"pass":0,"total":0}}
        
        # è¯¦ç»†çš„çº¦æŸç»Ÿè®¡å¤„ç†ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        for constraint in ['commonsense','hard']:
            if constraint == 'commonsense':
                constraint_statistic = commonsenseConstraint_statistic_processed
                key_list = ['is_valid_information_in_current_city','is_valid_information_in_sandbox','is_reasonalbe_visiting_city','is_valid_restaurants','is_valid_transportation','is_valid_attractions','is_valid_accommodation','is_not_absent']
            else:
                constraint_statistic = hardConstraint_statistic_processed
                key_list = ['valid_cost','valid_room_rule','valid_cuisine','valid_room_type','valid_transportation']
            
            for level in constraint_statistic:
                for day in constraint_statistic[level]:
                    for key in key_list:
                        if key in constraint_statistic[level][day]:
                            constraint_dis_record[constraint]['pass'] += constraint_statistic[level][day][key]['true']
                            if constraint == 'commonsense':
                                constraint_dis_record[constraint]['total'] += count_record[level][day]
                            else:
                                if key in ['valid_room_rule','valid_cuisine','valid_room_type','valid_transportation']:
                                    # åªæœ‰ medium å’Œ hard çº§åˆ«æœ‰ mapping_constraint_record
                                    if level in ['medium', 'hard']:
                                        constraint_dis_record[constraint]['total'] += mapping_constraint_record[level][day].get(key, 0)
                                    # easy çº§åˆ«ç›´æ¥ä½¿ç”¨ count_record
                                    else:
                                        constraint_dis_record[constraint]['total'] += count_record[level][day]
                                else:
                                    constraint_dis_record[constraint]['total'] += count_record[level][day]
        
        # è®¡ç®—å®è§‚é€šè¿‡ç‡
        for idx in range(0, min(len(self.query_data_list), len(plan_constraint_store))):
            if plan_constraint_store[idx]['commonsense_constraint']:
                final_commonsense_pass = True
                final_hardConstraint_pass = True
                
                # æ£€æŸ¥å¸¸è¯†çº¦æŸ
                for item in plan_constraint_store[idx]['commonsense_constraint']:
                    if plan_constraint_store[idx]['commonsense_constraint'][item][0] is not None and not plan_constraint_store[idx]['commonsense_constraint'][item][0]:
                        final_commonsense_pass = False
                        break
                
                # æ£€æŸ¥ç¡¬çº¦æŸ
                if plan_constraint_store[idx]['hard_constraint'] is None:
                    continue
                    
                for item in plan_constraint_store[idx]['hard_constraint']:
                    if plan_constraint_store[idx]['hard_constraint'][item][0] is not None and plan_constraint_store[idx]['hard_constraint'][item][0] == False:
                        final_hardConstraint_pass = False
                        break
                
                if final_commonsense_pass:
                    final_commonsense_cnt += 1
                if final_hardConstraint_pass:
                    final_hardConstraint_cnt += 1
                if final_commonsense_pass and final_hardConstraint_pass:
                    final_all_cnt += 1
        
        # è®¡ç®—æœ€ç»ˆç»“æœ
        result = {}
        
        # æ ¹æ®æ•°æ®é›†ç±»å‹è®¾ç½®æ€»æ•°
        if set_type == 'train':
            total_count = 45
            commonsense_total = 360
            hard_total = 105
        elif set_type == 'validation':
            total_count = 180
            commonsense_total = 1440
            hard_total = 420
        elif set_type == 'test':
            total_count = 1000
            commonsense_total = 8000
            hard_total = 2290
        else:
            # åŠ¨æ€è®¡ç®—
            total_count = min(len(self.query_data_list), len(tested_plans))
            commonsense_total = constraint_dis_record['commonsense']['total'] or total_count * 8
            hard_total = constraint_dis_record['hard']['total'] or 1
        
        result['Delivery Rate'] = delivery_cnt / total_count if total_count > 0 else 0
        result['Commonsense Constraint Micro Pass Rate'] = constraint_dis_record['commonsense']['pass'] / commonsense_total if commonsense_total > 0 else 0
        result['Commonsense Constraint Macro Pass Rate'] = final_commonsense_cnt / total_count if total_count > 0 else 0
        result['Hard Constraint Micro Pass Rate'] = constraint_dis_record['hard']['pass'] / hard_total if hard_total > 0 else 0
        result['Hard Constraint Macro Pass Rate'] = final_hardConstraint_cnt / total_count if total_count > 0 else 0
        result['Final Pass Rate'] = final_all_cnt / total_count if total_count > 0 else 0
        
        # è¯¦ç»†ç»“æœ
        remap_commonsense_constraint_record, remap_hard_constraint_record = self.paper_term_mapping(
            commonsenseConstraint_statistic_processed, hardConstraint_statistic_processed)
        
        detailed_scores = {
            "Commonsense Constraint": remap_commonsense_constraint_record,
            "Hard Constraint": remap_hard_constraint_record
        }
        
        return result, detailed_scores
    
    def statistics(self, constraint_statistic):
        """ç»Ÿè®¡çº¦æŸç»“æœ"""
        result = {level: {day: {} for day in constraint_statistic[level]} for level in constraint_statistic}
        
        for level, days in constraint_statistic.items():
            for day, dicts in days.items():
                for dct in dicts:
                    if dct:
                        for key, data in dct.items():
                            true_count = data.count(True) if isinstance(data, list) else (1 if data[0] else 0)
                            false_count = data.count(False) if isinstance(data, list) else (0 if data[0] else 1)
                            if key not in result[level][day]:
                                result[level][day][key] = {"true": 0, "false": 0}
                            result[level][day][key]["true"] += true_count
                            result[level][day][key]["false"] += false_count
        
        return result
    
    def paper_term_mapping(self, commonsense_constraint_record, hard_constraint_record):
        """æ˜ å°„è®ºæ–‡æœ¯è¯­"""
        mapping_dict = {
            'is_valid_information_in_current_city': 'Within Current City',
            'is_valid_information_in_sandbox': 'Within Sandbox',
            'is_reasonalbe_visiting_city': 'Reasonable City Route',
            'is_valid_restaurants': 'Diverse Restaurants',
            'is_valid_transportation': 'Non-conf. Transportation',
            'is_valid_attractions': 'Diverse Attractions',
            'is_valid_accommodation': 'Minimum Nights Stay',
            'is_not_absent': 'Complete Information',
            'valid_cost': 'Budget',
            'valid_room_rule': 'Room Rule',
            'valid_cuisine': 'Cuisine',
            'valid_room_type': 'Room Type',
            'valid_transportation': 'Transportation'
        }
        
        remap_commonsense = {level: {day: {} for day in [3,5,7]} for level in ['easy','medium','hard']}
        remap_hard = {level: {day: {} for day in [3,5,7]} for level in ['easy','medium','hard']}
        
        for level in commonsense_constraint_record:
            for day in commonsense_constraint_record[level]:
                remap_commonsense[level][day] = {
                    mapping_dict.get(key, key): val 
                    for key, val in commonsense_constraint_record[level][day].items()
                }
                remap_hard[level][day] = {
                    mapping_dict.get(key, key): val 
                    for key, val in hard_constraint_record[level][day].items()
                }
        
        return remap_commonsense, remap_hard
    
    def load_line_json_data(self, filename):
        """åŠ è½½ JSONL æ–‡ä»¶"""
        data = []
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f.read().strip().split('\n'):
                if line.strip():
                    unit = json.loads(line)
                    data.append(unit)
        return data
    
    def save_results(self, scores, detailed_scores):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        print("\nğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ...")
        
        # ä»æäº¤æ–‡ä»¶è·¯å¾„ç”Ÿæˆç»“æœæ–‡ä»¶å
        base_name = os.path.basename(self.submission_file)
        if base_name.endswith('.jsonl'):
            base_name = base_name[:-6]  # ç§»é™¤.jsonlåç¼€
        
        # åˆ›å»ºç»“æœç›®å½•
        result_dir = os.path.join(os.path.dirname(self.submission_file), 'results')
        os.makedirs(result_dir, exist_ok=True)
        
        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æ ¼å¼åŒ–åˆ†æ•°ç»“æœä¸ºç™¾åˆ†æ¯”
        formatted_scores = {}
        for key in scores:
            formatted_scores[key] = f"{float(int(scores[key]*10000))/100}%"
        
        # ä¿å­˜åˆå¹¶ç»“æœ
        combined_results = {
            'scores': formatted_scores,
            'detailed_scores': detailed_scores,
            'evaluation_file': self.submission_file,
            'set_type': self.args.set_type,
            'model_name': self.args.model_name,
            'strategy': self.args.strategy,
            'mode': self.args.mode,
            'timestamp': timestamp
        }
        
        combined_filename = f"{base_name}_results_{timestamp}.json"
        combined_path = os.path.join(result_dir, combined_filename)
        
        with open(combined_path, 'w', encoding='utf-8') as f:
            json.dump(combined_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {combined_path}")
        return combined_path
    
    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹"""
        print("ğŸš€ å¼€å§‹é›†æˆè¯„ä¼°æµç¨‹...")
        print("="*60)
        
        start_time = time.time()
        
        try:
            # åŠ è½½æ•°æ®é›†
            self.load_dataset()
            
            # æ­¥éª¤1: è§£æ
            if not self.step1_parsing():
                print("âŒ æ­¥éª¤1å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ")
                return False
            
            # æ­¥éª¤2: æå–
            if not self.step2_element_extraction():
                print("âŒ æ­¥éª¤2å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ")
                return False
            
            # æ­¥éª¤3: åˆå¹¶ï¼ˆç¬¬ä¸€æ¬¡å†™å…¥ï¼‰
            if not self.step3_combination():
                print("âŒ æ­¥éª¤3å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ")
                return False
            
            # æ­¥éª¤4: è¯„ä¼°ï¼ˆç¬¬äºŒæ¬¡å†™å…¥ï¼‰
            success, scores, detailed_scores = self.step4_evaluation()
            if not success:
                print("âŒ æ­¥éª¤4å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ")
                return False
            
            end_time = time.time()
            total_time = end_time - start_time
            
            print("\n" + "="*60)
            print("âœ… é›†æˆè¯„ä¼°å®Œæˆ!")
            print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’")
            print("="*60)
            
            return True
            
        except Exception as e:
            print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            traceback.print_exc()
            return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="é›†æˆè¯„ä¼°è„šæœ¬")
    parser.add_argument("--set_type", type=str, default="validation", choices=["train", "validation", "test"],
                       help="æ•°æ®é›†ç±»å‹")
    parser.add_argument("--model_name", type=str, default="Qwen2.5-72B-Instruct",
                       help="æ¨¡å‹åç§°")
    parser.add_argument("--strategy", type=str, default="direct", 
                       choices=["direct", "cot", "react", "evoagent"],
                       help="ç­–ç•¥åç§°")
    parser.add_argument("--mode", type=str, default="sole-planning", 
                       choices=["two-stage", "sole-planning"],
                       help="æ¨¡å¼")
    parser.add_argument("--output_dir", type=str, default="/Users/liuxiansheng/Agent/myevoagent/output",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--tmp_dir", type=str, default=None,
                       help="ä¸´æ—¶æ–‡ä»¶ç›®å½•")
    parser.add_argument("--submission_file_dir", type=str, default=None,
                       help="æäº¤æ–‡ä»¶ç›®å½•")
    parser.add_argument("--test_mode", action="store_true",
                       help="æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰5ä¸ªè®¡åˆ’")
    parser.add_argument("--limit", type=int, default=None,
                       help="é™åˆ¶å¤„ç†çš„è®¡åˆ’æ•°é‡")
    
    args = parser.parse_args()
    
    print("ğŸ“‹ è¯„ä¼°é…ç½®:")
    print(f"  æ•°æ®é›†ç±»å‹: {args.set_type}")
    print(f"  æ¨¡å‹åç§°: {args.model_name}")
    print(f"  ç­–ç•¥: {args.strategy}")
    print(f"  æ¨¡å¼: {args.mode}")
    print(f"  è¾“å‡ºç›®å½•: {args.output_dir}")
    print()
    
    # åˆ›å»ºè¯„ä¼°å™¨å¹¶è¿è¡Œ
    evaluator = IntegratedEvaluator(args)
    success = evaluator.run()
    
    if success:
        print("ğŸ‰ è¯„ä¼°æˆåŠŸå®Œæˆ!")
        return 0
    else:
        print("ğŸ’¥ è¯„ä¼°å¤±è´¥!")
        return 1


if __name__ == "__main__":
    exit(main())
