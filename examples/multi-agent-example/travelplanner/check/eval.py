#!/usr/bin/env python3
"""
é›†æˆè¯„ä¼°è„šæœ¬ - åˆå¹¶å››ä¸ªè¯„ä¼°æ­¥éª¤ä¸ºä¸€ä¸ªæ–‡ä»¶
åŒ…å«ï¼šparsing, element_extraction, combination, evaluation
"""

import os
import sys
import json
import argparse
import time
from tqdm import tqdm
from datetime import datetime
from typing import List, Dict, Any
import traceback

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥å¿…è¦æ¨¡å—
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))


# å¯¼å…¥å¿…è¦çš„æ¨¡å—
try:
    from datasets import load_dataset
    from openai_request import build_plan_format_conversion_prompt, prompt_chatgpt
    from commonsense_constraint import evaluation as commonsense_eval
    from hard_constraint import evaluation as hard_eval
except ImportError as e:
    print(f"Warning: Import error: {e}")
    print("Some functionality may not be available")


class IntegratedEvaluator:
    """é›†æˆè¯„ä¼°å™¨ - æ‰§è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹"""
    
    def __init__(self, args):
        self.args = args
        self.query_data_list = None  # çœŸæ­£çš„æŸ¥è¯¢æ•°æ®ä»æ•°æ®é›†ä¸­åŠ è½½
        self.model_outputs = None    # æ¨¡å‹è¾“å‡ºæ•°æ®ä»è¾“å…¥æ–‡ä»¶ä¸­åŠ è½½
        self.parsed_results = []
        
        # ä»è¾“å…¥æ–‡ä»¶ä¸­åŠ è½½æ¨¡å‹è¾“å‡ºæ•°æ®
        self.load_model_outputs()
        
        # ä»æ•°æ®é›†ä¸­åŠ è½½çœŸæ­£çš„æŸ¥è¯¢æ•°æ®
        self.load_dataset()
        
        # è®¾ç½®è·¯å¾„
        self.setup_paths()
        
    def setup_paths(self):
        """è®¾ç½®æ‰€æœ‰å¿…è¦çš„è·¯å¾„"""
        # åŸºç¡€è·¯å¾„
        input_dir = os.path.dirname(self.args.input_file)
        self.results_dir = '../results'
        os.makedirs(self.results_dir, exist_ok=True)
        
        # ä¸´æ—¶æ–‡ä»¶è·¯å¾„
        self.tmp_dir = self.results_dir + '/parse'
        os.makedirs(self.tmp_dir, exist_ok=True)
        
        # æäº¤æ–‡ä»¶è·¯å¾„
        self.submission_dir = self.results_dir + '/sub'
        os.makedirs(self.submission_dir, exist_ok=True)
        
        # ä»è¾“å…¥æ–‡ä»¶åæ´¾ç”Ÿè¾“å‡ºæ–‡ä»¶å
        base_name = os.path.basename(self.args.input_file)
        if base_name.endswith('.json'):
            base_name = base_name[:-5]
        
        self.submission_file = os.path.join(self.submission_dir, f'{base_name}_submission.jsonl')
        
        print(f"è¾“å…¥æ–‡ä»¶: {self.args.input_file}")
        print(f"ç»“æœç›®å½•: {self.results_dir}")
        print(f"æœ€ç»ˆæäº¤æ–‡ä»¶: {self.submission_file}")

    def load_model_outputs(self):
        """ä»å•ä¸ªè¾“å…¥æ–‡ä»¶åŠ è½½æ¨¡å‹è¾“å‡ºæ•°æ®"""
        print(f"ğŸ“ ä»è¾“å…¥æ–‡ä»¶åŠ è½½æ¨¡å‹è¾“å‡ºæ•°æ®: {self.args.input_file}")
        try:
            with open(self.args.input_file, 'r', encoding='utf-8') as f:
                self.model_outputs = json.load(f)
            print(f"æˆåŠŸåŠ è½½ {len(self.model_outputs)} ä¸ªæ¨¡å‹è¾“å‡º")
        except Exception as e:
            print(f"åŠ è½½è¾“å…¥æ–‡ä»¶å¤±è´¥: {e}")
            raise

    def load_dataset(self):
        """ä»æ•°æ®é›†ä¸­åŠ è½½çœŸæ­£çš„æŸ¥è¯¢æ•°æ®"""
        print("ğŸ“ ä»æ•°æ®é›†åŠ è½½æŸ¥è¯¢æ•°æ®...")
        
        # ç¡®å®šæ•°æ®é›†ç±»å‹
        if 'train' in self.args.input_file:
            set_type = 'train'
        elif 'validation' in self.args.input_file:
            set_type = 'validation'
        elif 'test' in self.args.input_file:
            set_type = 'test'
        else:
            print("è­¦å‘Š: æ— æ³•ä»æ–‡ä»¶åç¡®å®šæ•°æ®é›†ç±»å‹ã€‚é»˜è®¤ä¸º 'validation'ã€‚")
            set_type = 'validation'
            
        self.set_type = set_type
        print(f"æ¨æ–­çš„æ•°æ®é›†ç±»å‹: {self.set_type}")
        
        # æ„å»ºæ•°æ®é›†æ–‡ä»¶è·¯å¾„
        self.dataset_file = f'../../../../dataset/travelplanner/{set_type}/travelplanner_{set_type}_dataset.json'
        
        try:
            with open(self.dataset_file, 'r', encoding='utf-8') as f:
                self.query_data_list = json.load(f)
            print(f"æˆåŠŸåŠ è½½ {len(self.query_data_list)} ä¸ªæ•°æ®é¡¹")
        except Exception as e:
            print(f"åŠ è½½æ•°æ®é›†æ–‡ä»¶å¤±è´¥: {e}")
            # å¦‚æœæœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä» HuggingFace ä¸‹è½½
            print("å°è¯•ä» HuggingFace ä¸‹è½½æ•°æ®é›†...")
            try:
                if set_type == 'validation':
                    hf_dataset = load_dataset('osunlp/TravelPlanner', 'validation')['validation']
                elif set_type == 'test':
                    hf_dataset = load_dataset('osunlp/TravelPlanner', 'test')['test']
                elif set_type == 'train':
                    hf_dataset = load_dataset('osunlp/TravelPlanner', 'train')['train']
                
                # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
                self.query_data_list = [dict(item) for item in hf_dataset]
                print(f"ä» HuggingFace ä¸‹è½½ {len(self.query_data_list)} ä¸ªæ•°æ®é¡¹")
                
                # ä¿å­˜åˆ°æœ¬åœ°ä»¥å¤‡åç”¨
                os.makedirs(os.path.dirname(self.dataset_file), exist_ok=True)
                with open(self.dataset_file, 'w', encoding='utf-8') as f:
                    json.dump(self.query_data_list, f, indent=2, ensure_ascii=False)
                print(f"æ•°æ®é›†å·²ä¿å­˜åˆ°æœ¬åœ°: {self.dataset_file}")
                
            except Exception as e2:
                print(f"ä» HuggingFace ä¸‹è½½æ•°æ®é›†ä¹Ÿå¤±è´¥: {e2}")
                raise
    
    def step1_parsing(self):
        """æ­¥éª¤1: è§£æç”Ÿæˆçš„è®¡åˆ’æ–‡æœ¬"""
        print("\nğŸ”„ æ­¥éª¤ 1/4: è§£æè®¡åˆ’æ–‡æœ¬...")
        
        # å‡†å¤‡è¾“å‡ºæ–‡ä»¶è·¯å¾„
        base_name = os.path.basename(self.args.input_file).replace('.json', '')
        tmp_output_file = os.path.join(self.tmp_dir, f'{base_name}_parsed.txt')
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å®Œæˆè§£æ
        if os.path.exists(tmp_output_file) and os.path.getsize(tmp_output_file) > 0:
            print(f"âœ… è§£ææ–‡ä»¶å·²å­˜åœ¨ä¸”éç©º: {tmp_output_file}")
            print("è·³è¿‡æ­¥éª¤1ï¼Œç›´æ¥åŠ è½½å·²æœ‰çš„è§£æç»“æœ...")
            
            # åŠ è½½å·²æœ‰çš„è§£æç»“æœ
            try:
                with open(tmp_output_file, 'r', encoding='utf-8') as f:
                    lines = f.read().strip().split('\n')
                    self.parsed_results = []
                    for line in lines:
                        if '\t' in line:
                            # æ ¼å¼: idx\tresult
                            parts = line.split('\t', 1)
                            if len(parts) == 2:
                                self.parsed_results.append(parts[1])
                            else:
                                self.parsed_results.append("")
                        else:
                            self.parsed_results.append(line)
                
                print(f"æˆåŠŸåŠ è½½ {len(self.parsed_results)} ä¸ªè§£æç»“æœ")
                return True
                
            except Exception as e:
                print(f"åŠ è½½å·²æœ‰è§£æç»“æœå¤±è´¥: {e}")
                print("é‡æ–°æ‰§è¡Œè§£ææ­¥éª¤...")
                # åˆ é™¤æŸåçš„æ–‡ä»¶ï¼Œé‡æ–°è§£æ
                os.remove(tmp_output_file)
        
        try:
            # ä½¿ç”¨åŸæœ‰çš„prefixæ ¼å¼ï¼Œä½†é€‚é…æˆ‘ä»¬çš„æ•°æ®ç»“æ„
            prefix = """Please assist me in extracting valid information from a given natural language text and reconstructing it in JSON format, as demonstrated in the following example. If transportation details indicate a journey from one city to another (e.g., from A to B), the 'current_city' should be updated to the destination city (in this case, B). Use a ';' to separate different attractions, with each attraction formatted as 'Name, City'. If there's information about transportation, ensure that the 'current_city' aligns with the destination mentioned in the transportation details (i.e., the current city should follow the format 'from A to B'). Also, ensure that all flight numbers and costs are followed by a colon (i.e., 'Flight Number:' and 'Cost:'), consistent with the provided example. Each item should include ['day', 'current_city', 'transportation', 'breakfast', 'attraction', 'lunch', 'dinner', 'accommodation']. Replace non-specific information like 'eat at home/on the road' with '-'. Additionally, delete any '$' symbols.
-----EXAMPLE-----
 [{{
        "day": 1,
        "current_city": "from Dallas to Peoria",
        "transportation": "Flight Number: 4044830, from Dallas to Peoria, Departure Time: 13:10, Arrival Time: 15:01",
        "breakfast": "-",
        "attraction": "Peoria Historical Society, Peoria;Peoria Holocaust Memorial, Peoria;",
        "lunch": "-",
        "dinner": "Tandoor Ka Zaika, Peoria",
        "accommodation": "Bushwick Music Mansion, Peoria"
    }},
    {{
        "day": 2,
        "current_city": "Peoria",
        "transportation": "-",
        "breakfast": "Tandoor Ka Zaika, Peoria",
        "attraction": "Peoria Riverfront Park, Peoria;The Peoria PlayHouse, Peoria;Glen Oak Park, Peoria;",
        "lunch": "Cafe Hashtag LoL, Peoria",
        "dinner": "The Curzon Room - Maidens Hotel, Peoria",
        "accommodation": "Bushwick Music Mansion, Peoria"
    }},
    {{
        "day": 3,
        "current_city": "from Peoria to Dallas",
        "transportation": "Flight Number: 4045904, from Peoria to Dallas, Departure Time: 07:09, Arrival Time: 09:20",
        "breakfast": "-",
        "attraction": "-",
        "lunch": "-",
        "dinner": "-",
        "accommodation": "-"
    }}]
-----EXAMPLE END-----
"""
            
            # ç›´æ¥ä»åŠ è½½çš„æ¨¡å‹è¾“å‡ºæ•°æ®æ„å»ºæç¤ºï¼Œä½¿ç”¨åŸæœ‰çš„æ ¼å¼
            prompt_list = []
            for item in self.model_outputs:
                model_output = item.get("model_output", "")
                if model_output and model_output not in ["", "Max Token Length Exceeded."]:
                    prompt = prefix + "Text:\n"+model_output+"\nJSON:\n"
                    prompt_list.append(prompt)
                else:
                    prompt_list.append("") # ä¿ç•™ç©ºå­—ç¬¦ä¸²ä»¥ç»´æŒç´¢å¼•å¯¹åº”

            # æ¸…é™¤æ—§çš„è§£æç»“æœæ–‡ä»¶
            if os.path.exists(tmp_output_file):
                os.remove(tmp_output_file)

            total_price = 0
            self.parsed_results = []
            
            print(f"å¤„ç† {len(prompt_list)} ä¸ªè®¡åˆ’...")
            for idx, prompt in enumerate(tqdm(prompt_list)):
                if not prompt:
                    result = "" # å¯¹äºæ²¡æœ‰å†…å®¹çš„è®¡åˆ’ï¼Œè§£æç»“æœä¹Ÿä¸ºç©º
                    self.parsed_results.append(result)
                    # ä¸å†å†™å…¥ç©ºç»“æœåˆ°æ–‡ä»¶ï¼Œé¿å…æ··æ·†
                    continue
                
                try:
                    # è°ƒç”¨ LLM è¿›è¡Œè§£æ
                    env_model_name = "Qwen2.5-72B-Instruct"
                    results, _, price = prompt_chatgpt(
                        "You are a helpful assistant.", 
                        index=idx, 
                        save_path=tmp_output_file,
                        user_input=prompt, 
                        model_name=env_model_name, 
                        temperature=0
                    )
                    total_price += price
                    self.parsed_results.append(results)
                    
                except Exception as e:
                    print(f"è§£æç¬¬ {idx} ä¸ªè®¡åˆ’æ—¶å‡ºé”™: {e}")
                    error_result = f"ERROR: {str(e)}"
                    self.parsed_results.append(error_result)
                    with open(tmp_output_file, 'a+', encoding='utf-8') as f:
                        f.write(f"{idx}\t{error_result}\n")
            
            print(f"è§£æå®Œæˆï¼Œæ€»è´¹ç”¨: ${total_price}")
            return True
            
        except Exception as e:
            print(f"è§£ææ­¥éª¤å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def step2_element_extraction(self):
        """æ­¥éª¤2: æå–ç»“æ„åŒ–æ•°æ®"""
        print("\nğŸ¯ æ­¥éª¤ 2/4: æå–ç»“æ„åŒ–æ•°æ®...")
        
        try:
            print(f"å¤„ç† {len(self.model_outputs)} ä¸ªæå–ä»»åŠ¡...")
            success_count = 0
            
            for idx, item in enumerate(tqdm(self.model_outputs)):
                # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„æ¨¡å‹è¾“å‡ºå’Œè§£æç»“æœ
                model_output = item.get("model_output", "")
                if model_output and model_output not in ["", "Max Token Length Exceeded."] and idx < len(self.parsed_results):
                    try:
                        # æå– JSON å†…å®¹
                        result_text = self.parsed_results[idx]
                        if '```json' in result_text:
                            result = result_text.split('```json')[1].split('```')[0]
                        else:
                            result = result_text
                        
                        # è§£æ JSON - æœŸæœ›ç›´æ¥å¾—åˆ°è®¡åˆ’æ•°ç»„ï¼Œè€Œä¸æ˜¯åŒ…å«"plan"é”®çš„å¯¹è±¡
                        try:
                            parsed_result = json.loads(result.strip())
                        except json.JSONDecodeError:
                            # å¦‚æœ json.loads å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ eval ä½œä¸ºå¤‡é€‰
                            parsed_result = eval(result.strip())
                        
                        # éªŒè¯è§£æç»“æœçš„æ ¼å¼ - åº”è¯¥ç›´æ¥æ˜¯ä¸€ä¸ªåˆ—è¡¨
                        if isinstance(parsed_result, list) and len(parsed_result) > 0:
                            # æ£€æŸ¥åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¦åŒ…å«å¿…è¦å­—æ®µ
                            if isinstance(parsed_result[0], dict) and 'current_city' in parsed_result[0]:
                                item['parsed_plan'] = parsed_result
                                success_count += 1
                            else:
                                print(f"ç¬¬ {idx} ä¸ªè®¡åˆ’æ ¼å¼é”™è¯¯: åˆ—è¡¨å…ƒç´ ç¼ºå°‘å¿…è¦å­—æ®µ")
                                item['parsed_plan'] = None
                        elif isinstance(parsed_result, dict) and 'plan' in parsed_result:
                            # å¦‚æœæ„å¤–å¾—åˆ°äº†åŒ…å«"plan"é”®çš„å¯¹è±¡ï¼Œæå–å…¶ä¸­çš„è®¡åˆ’
                            plan_data = parsed_result['plan']
                            if isinstance(plan_data, list) and len(plan_data) > 0:
                                item['parsed_plan'] = plan_data
                                success_count += 1
                            else:
                                print(f"ç¬¬ {idx} ä¸ªè®¡åˆ’æ ¼å¼é”™è¯¯: plan åº”è¯¥æ˜¯éç©ºåˆ—è¡¨")
                                item['parsed_plan'] = None
                        else:
                            print(f"ç¬¬ {idx} ä¸ªè®¡åˆ’æ ¼å¼é”™è¯¯: åº”è¯¥æ˜¯åˆ—è¡¨æˆ–åŒ…å«'plan'é”®çš„å¯¹è±¡")
                            item['parsed_plan'] = None
                        
                    except Exception as e:
                        print(f"è§£æç¬¬ {idx} ä¸ªç»“æœæ—¶å‡ºé”™: {e}")
                        item['parsed_plan'] = None # æ ‡è®°è§£æå¤±è´¥
                else:
                    item['parsed_plan'] = None # æ²¡æœ‰æœ‰æ•ˆè¾“å‡ºæˆ–è§£æç»“æœ
            
            print(f"æå–å®Œæˆï¼ŒæˆåŠŸå¤„ç†: {success_count}/{len(self.model_outputs)}")
            return True
            
        except Exception as e:
            print(f"æå–æ­¥éª¤å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def step3_combination(self):
        """æ­¥éª¤3: åˆå¹¶ä¸ºè¯„ä¼°æ ¼å¼"""
        print("\nğŸ“Š æ­¥éª¤ 3/4: åˆå¹¶æ–‡ä»¶...")
        
        # æ£€æŸ¥æäº¤æ–‡ä»¶æ˜¯å¦å·²ç»å­˜åœ¨ä¸”éç©º
        if os.path.exists(self.submission_file) and os.path.getsize(self.submission_file) > 0:
            print(f"âœ… æäº¤æ–‡ä»¶å·²å­˜åœ¨ä¸”éç©º: {self.submission_file}")
            print("è·³è¿‡æ­¥éª¤3ï¼Œç›´æ¥åŠ è½½å·²æœ‰çš„æäº¤æ–‡ä»¶...")
            
            # åŠ è½½å·²æœ‰çš„æäº¤æ•°æ®
            try:
                self.submission_list = self.load_line_json_data(self.submission_file)
                print(f"æˆåŠŸåŠ è½½ {len(self.submission_list)} ä¸ªæäº¤é¡¹")
                return True
                
            except Exception as e:
                print(f"åŠ è½½å·²æœ‰æäº¤æ–‡ä»¶å¤±è´¥: {e}")
                print("é‡æ–°æ‰§è¡Œåˆå¹¶æ­¥éª¤...")
                # åˆ é™¤æŸåçš„æ–‡ä»¶ï¼Œé‡æ–°åˆå¹¶
                os.remove(self.submission_file)
        
        try:
            self.submission_list = []
            
            # ç¡®ä¿ä¸¤ä¸ªæ•°æ®é›†çš„é•¿åº¦ä¸€è‡´
            min_length = min(len(self.query_data_list), len(self.model_outputs))
            print(f"åˆå¹¶ {min_length} ä¸ªè®¡åˆ’...")
            
            for idx in tqdm(range(min_length)):
                query_item = self.query_data_list[idx]
                model_item = self.model_outputs[idx]
                
                # æ·»åŠ åˆ°æäº¤åˆ—è¡¨ï¼Œä½¿ç”¨çœŸæ­£çš„æŸ¥è¯¢æ•°æ®å’Œè§£æåçš„è®¡åˆ’
                submission_item = {
                    "idx": query_item.get('idx', idx + 1), # ä½¿ç”¨çœŸæ­£æŸ¥è¯¢æ•°æ®çš„idx
                    "query": query_item['query'],  # ä½¿ç”¨çœŸæ­£çš„æŸ¥è¯¢æ•°æ®
                    "plan": model_item.get('parsed_plan', None) # ä½¿ç”¨æ¨¡å‹è¾“å‡ºä¸­è§£æå¥½çš„è®¡åˆ’
                }
                self.submission_list.append(submission_item)
            
            # å†™å…¥æäº¤æ–‡ä»¶
            print(f"ğŸ’¾ å†™å…¥æäº¤æ–‡ä»¶: {self.submission_file}")
            with open(self.submission_file, 'w', encoding='utf-8') as w:
                for unit in self.submission_list:
                    output = json.dumps(unit, ensure_ascii=False)
                    w.write(output + "\n")
            
            print(f"åˆå¹¶å®Œæˆï¼Œç”Ÿæˆ {len(self.submission_list)} ä¸ªæäº¤é¡¹")
            return True
            
        except Exception as e:
            print(f"åˆå¹¶æ­¥éª¤å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def step4_evaluation(self):
        """æ­¥éª¤4: è¯„ä¼°ç»“æœ"""
        print("\nğŸ“ˆ æ­¥éª¤ 4/4: è¯„ä¼°ç»“æœ...")
        try:
            # ä½¿ç”¨ç°æœ‰çš„è¯„ä¼°å‡½æ•°
            scores, detailed_scores, per_plan_results = self.eval_score_with_per_plan(self.set_type, self.submission_file)

            # è¾“å‡ºç»“æœ
            print("\n" + "="*60)
            print("ğŸ¯ è¯„ä¼°ç»“æœ:")
            print("="*60)
            for key in scores:
                percentage = float(int(scores[key]*10000))/100
                print(f"{key}: {percentage}%")

            print("\n" + "="*60)
            print("ğŸ“Š è¯¦ç»†ç»“æœ:")
            print("="*60)
            print(json.dumps(detailed_scores, indent=2, ensure_ascii=False))

            # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
            self.save_results(scores, detailed_scores)

            # æ–°å¢ï¼šä¿å­˜æ¯æ¡è§„åˆ’çš„è¯¦ç»†è¯„ä»·æŒ‡æ ‡åˆ° jsonl æ–‡ä»¶
            self.save_per_plan_results(per_plan_results)

            return True, scores, detailed_scores
        except Exception as e:
            print(f"è¯„ä¼°æ­¥éª¤å¤±è´¥: {e}")
            traceback.print_exc()
            return False, None, None
        
    def save_per_plan_results(self, per_plan_results):
        """ä¿å­˜æ¯æ¡è§„åˆ’çš„è¯¦ç»†è¯„ä»·æŒ‡æ ‡åˆ° jsonl æ–‡ä»¶"""
        print("\nğŸ’¾ ä¿å­˜æ¯æ¡è§„åˆ’çš„è¯¦ç»†è¯„ä»·æŒ‡æ ‡...")
        base_name = os.path.basename(self.args.input_file)
        if base_name.endswith('.json'):
            base_name = base_name[:-5]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        per_plan_filename = f"{base_name}_per_results_{timestamp}.jsonl"
        per_plan_path = os.path.join(self.results_dir, per_plan_filename)
        with open(per_plan_path, 'w', encoding='utf-8') as f:
            for item in per_plan_results:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
        print(f"ğŸ“ æ¯æ¡è§„åˆ’è¯¦ç»†è¯„ä»·å·²ä¿å­˜åˆ°: {per_plan_path}")
        return per_plan_path
    
    def eval_score_with_per_plan(self, set_type: str, file_path: str):
        """è¯„ä¼°åˆ†æ•°å¹¶è¿”å›æ¯æ¡è§„åˆ’çš„è¯¦ç»†è¯„ä»·æŒ‡æ ‡"""
        # åŠ è½½æµ‹è¯•è®¡åˆ’
        tested_plans = self.load_line_json_data(file_path)

        # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
        hardConstraint_statistic = {level: {day: [] for day in [3,5,7]} for level in ['easy','medium','hard']}
        commonsenseConstraint_statistic = {level: {day: [] for day in [3,5,7]} for level in ['easy','medium','hard']}

        delivery_cnt = 0
        plan_constraint_store = []
        per_plan_results = []

        print(f"è¯„ä¼° {min(len(self.query_data_list), len(tested_plans))} ä¸ªè®¡åˆ’...")

        for idx in range(0, min(len(self.query_data_list), len(tested_plans))):
            query_data = self.query_data_list[idx]
            tested_plan = tested_plans[idx]
            # æ•°æ®ç±»å‹è½¬æ¢
            if type(query_data) == str:
                query_data = eval(query_data)
            if type(tested_plan) == str:
                tested_plan = eval(tested_plan)
            if type(query_data['local_constraint']) == str:
                query_data['local_constraint'] = eval(query_data['local_constraint'])

            # å¸¸è¯†çº¦æŸè¯„ä¼°
            if tested_plan['plan']:
                delivery_cnt += 1
                commonsense_info_box = commonsense_eval(query_data, tested_plan['plan'])
            else:
                commonsense_info_box = None

            # ç¡¬çº¦æŸè¯„ä¼°ï¼ˆåªæœ‰é€šè¿‡å¸¸è¯†çº¦æŸæ‰è¯„ä¼°ï¼‰
            if commonsense_info_box and commonsense_info_box['is_not_absent'][0] and commonsense_info_box['is_valid_information_in_sandbox'][0]:
                hard_info_box = hard_eval(query_data, tested_plan['plan'])
            else:
                hard_info_box = None

            plan_constraint_store.append({
                'commonsense_constraint': commonsense_info_box,
                'hard_constraint': hard_info_box
            })

            commonsenseConstraint_statistic[query_data['level']][query_data['days']].append(commonsense_info_box)
            hardConstraint_statistic[query_data['level']][query_data['days']].append(hard_info_box)

            # æ–°å¢ï¼šæ¯æ¡è§„åˆ’çš„è¯¦ç»†è¯„ä»·æŒ‡æ ‡
            per_plan_results.append({
                'idx': query_data.get('idx', idx),
                'query': query_data.get('query', None),
                'plan': tested_plan.get('plan', None),
                'commonsense_constraint': commonsense_info_box,
                'hard_constraint': hard_info_box
            })

        # å¤„ç†çº¦æŸç»Ÿè®¡
        constraint_record = {key: {day: {'house rule':0, 'cuisine':0, 'room type':0, 'transportation':0} for day in [3,5,7]} for key in ['medium','hard']}
        constraint_mapping = {'house rule':'valid_room_rule','cuisine':'valid_cuisine','room type':'valid_room_type','transportation':'valid_transportation'}
        mapping_constraint_record = {key: {day: {'valid_room_rule':0, 'valid_cuisine':0, 'valid_room_type':0, 'valid_transportation':0} for day in [3,5,7]} for key in ['medium','hard']}
        count_record = {key:{day:0 for day in [3,5,7]} for key in ['easy','medium','hard']}

        for unit in self.query_data_list:
            if type(unit) == str:
                unit = eval(unit)
            if type(unit['local_constraint']) == str:
                unit['local_constraint'] = eval(unit['local_constraint'])
            count_record[unit['level']][unit['days']] += 1
            for key in constraint_record['medium'][3]:
                try:
                    if unit['local_constraint'][key] != None:
                        if unit['level'] in constraint_record:  # åªå¤„ç† medium å’Œ hard çº§åˆ«
                            constraint_record[unit['level']][unit['days']][key] += 1
                            mapping_constraint_record[unit['level']][unit['days']][constraint_mapping[key]] += 1
                except Exception:
                    continue

        # ç»Ÿè®¡å¤„ç†
        commonsenseConstraint_statistic_processed = self.statistics(commonsenseConstraint_statistic)
        hardConstraint_statistic_processed = self.statistics(hardConstraint_statistic)

        # è®¡ç®—æœ€ç»ˆåˆ†æ•°
        final_all_cnt = 0
        final_commonsense_cnt = 0
        final_hardConstraint_cnt = 0

        constraint_dis_record = {"commonsense":{"pass":0,"total":0},"hard":{"pass":0,"total":0}}

        # è¯¦ç»†çš„çº¦æŸç»Ÿè®¡å¤„ç†ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        for constraint in ['commonsense','hard']:
            if constraint == 'commonsense':
                constraint_statistic = commonsenseConstraint_statistic_processed
                key_list = ['is_valid_information_in_current_city','is_valid_information_in_sandbox','is_reasonalbe_visiting_city','is_valid_restaurants','is_valid_transportation','is_valid_attractions','is_valid_accommodation','is_not_absent']
            else:
                constraint_statistic = hardConstraint_statistic_processed
                key_list = ['valid_cost','valid_room_rule','valid_cuisine','valid_room_type','valid_transportation']
            for level in constraint_statistic:
                for day in constraint_statistic[level]:
                    for key in key_list:
                        if key in constraint_statistic[level][day]:
                            constraint_dis_record[constraint]['pass'] += constraint_statistic[level][day][key]['true']
                            if constraint == 'commonsense':
                                constraint_dis_record[constraint]['total'] += count_record[level][day]
                            else:
                                if key in ['valid_room_rule','valid_cuisine','valid_room_type','valid_transportation']:
                                    if level in ['medium', 'hard']:
                                        constraint_dis_record[constraint]['total'] += mapping_constraint_record[level][day].get(key, 0)
                                    else:
                                        constraint_dis_record[constraint]['total'] += count_record[level][day]
                                else:
                                    constraint_dis_record[constraint]['total'] += count_record[level][day]

        # è®¡ç®—å®è§‚é€šè¿‡ç‡
        for idx in range(0, min(len(self.query_data_list), len(plan_constraint_store))):
            if plan_constraint_store[idx]['commonsense_constraint']:
                final_commonsense_pass = True
                final_hardConstraint_pass = True
                for item in plan_constraint_store[idx]['commonsense_constraint']:
                    if plan_constraint_store[idx]['commonsense_constraint'][item][0] is not None and not plan_constraint_store[idx]['commonsense_constraint'][item][0]:
                        final_commonsense_pass = False
                        break
                if plan_constraint_store[idx]['hard_constraint'] is None:
                    continue
                for item in plan_constraint_store[idx]['hard_constraint']:
                    if plan_constraint_store[idx]['hard_constraint'][item][0] is not None and plan_constraint_store[idx]['hard_constraint'][item][0] == False:
                        final_hardConstraint_pass = False
                        break
                if final_commonsense_pass:
                    final_commonsense_cnt += 1
                if final_hardConstraint_pass:
                    final_hardConstraint_cnt += 1
                if final_commonsense_pass and final_hardConstraint_pass:
                    final_all_cnt += 1

        # è®¡ç®—æœ€ç»ˆç»“æœ
        result = {}
        if set_type == 'train':
            total_count = 45
            commonsense_total = 360
            hard_total = 105
        elif set_type == 'validation':
            total_count = 180
            commonsense_total = 1440
            hard_total = 420
        elif set_type == 'test':
            total_count = 1000
            commonsense_total = 8000
            hard_total = 2290
        else:
            total_count = min(len(self.query_data_list), len(tested_plans))
            commonsense_total = constraint_dis_record['commonsense']['total'] or total_count * 8
            hard_total = constraint_dis_record['hard']['total'] or 1

        result['Delivery Rate'] = delivery_cnt / total_count if total_count > 0 else 0
        result['Commonsense Constraint Micro Pass Rate'] = constraint_dis_record['commonsense']['pass'] / commonsense_total if commonsense_total > 0 else 0
        result['Commonsense Constraint Macro Pass Rate'] = final_commonsense_cnt / total_count if total_count > 0 else 0
        result['Hard Constraint Micro Pass Rate'] = constraint_dis_record['hard']['pass'] / hard_total if hard_total > 0 else 0
        result['Hard Constraint Macro Pass Rate'] = final_hardConstraint_cnt / total_count if total_count > 0 else 0
        result['Final Pass Rate'] = final_all_cnt / total_count if total_count > 0 else 0

        remap_commonsense_constraint_record, remap_hard_constraint_record = self.paper_term_mapping(
            commonsenseConstraint_statistic_processed, hardConstraint_statistic_processed)

        detailed_scores = {
            "Commonsense Constraint": remap_commonsense_constraint_record,
            "Hard Constraint": remap_hard_constraint_record
        }

        return result, detailed_scores, per_plan_results
    
    def eval_score(self, set_type: str, file_path: str):
        """è¯„ä¼°åˆ†æ•° - ä»åŸ eval.py ç§»æ¤çš„é€»è¾‘"""
        
        # åŠ è½½æµ‹è¯•è®¡åˆ’
        tested_plans = self.load_line_json_data(file_path)
        
        # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
        hardConstraint_statistic = {level: {day: [] for day in [3,5,7]} for level in ['easy','medium','hard']}
        commonsenseConstraint_statistic = {level: {day: [] for day in [3,5,7]} for level in ['easy','medium','hard']}
        
        delivery_cnt = 0
        plan_constraint_store = []
        
        print(f"è¯„ä¼° {min(len(self.query_data_list), len(tested_plans))} ä¸ªè®¡åˆ’...")
        
        for idx in tqdm(range(0, min(len(self.query_data_list), len(tested_plans)))):
            query_data = self.query_data_list[idx]
            tested_plan = tested_plans[idx]
            
            # æ•°æ®ç±»å‹è½¬æ¢
            if type(query_data) == str:
                query_data = eval(query_data)
            if type(tested_plan) == str:
                tested_plan = eval(tested_plan)
            if type(query_data['local_constraint']) == str:
                query_data['local_constraint'] = eval(query_data['local_constraint'])
            
            # å¸¸è¯†çº¦æŸè¯„ä¼°
            if tested_plan['plan']:
                delivery_cnt += 1
                commonsense_info_box = commonsense_eval(query_data, tested_plan['plan'])
            else:
                commonsense_info_box = None
            
            # ç¡¬çº¦æŸè¯„ä¼°ï¼ˆåªæœ‰é€šè¿‡å¸¸è¯†çº¦æŸæ‰è¯„ä¼°ï¼‰
            if commonsense_info_box and commonsense_info_box['is_not_absent'][0] and commonsense_info_box['is_valid_information_in_sandbox'][0]:
                hard_info_box = hard_eval(query_data, tested_plan['plan'])
            else:
                hard_info_box = None
            
            plan_constraint_store.append({
                'commonsense_constraint': commonsense_info_box,
                'hard_constraint': hard_info_box
            })
            
            commonsenseConstraint_statistic[query_data['level']][query_data['days']].append(commonsense_info_box)
            hardConstraint_statistic[query_data['level']][query_data['days']].append(hard_info_box)
        
        # å¤„ç†çº¦æŸç»Ÿè®¡
        constraint_record = {key: {day: {'house rule':0, 'cuisine':0, 'room type':0, 'transportation':0} for day in [3,5,7]} for key in ['medium','hard']}
        constraint_mapping = {'house rule':'valid_room_rule','cuisine':'valid_cuisine','room type':'valid_room_type','transportation':'valid_transportation'}
        mapping_constraint_record = {key: {day: {'valid_room_rule':0, 'valid_cuisine':0, 'valid_room_type':0, 'valid_transportation':0} for day in [3,5,7]} for key in ['medium','hard']}
        count_record = {key:{day:0 for day in [3,5,7]} for key in ['easy','medium','hard']}
        
        for unit in self.query_data_list:
            if type(unit) == str:
                unit = eval(unit)
            if type(unit['local_constraint']) == str:
                unit['local_constraint'] = eval(unit['local_constraint'])
                
            count_record[unit['level']][unit['days']] += 1
            for key in constraint_record['medium'][3]:
                try:
                    if unit['local_constraint'][key] != None:
                        if unit['level'] in constraint_record:  # åªå¤„ç† medium å’Œ hard çº§åˆ«
                            constraint_record[unit['level']][unit['days']][key] += 1
                            mapping_constraint_record[unit['level']][unit['days']][constraint_mapping[key]] += 1
                except Exception:
                    continue
        
        # ç»Ÿè®¡å¤„ç†
        commonsenseConstraint_statistic_processed = self.statistics(commonsenseConstraint_statistic)
        hardConstraint_statistic_processed = self.statistics(hardConstraint_statistic)
        
        # è®¡ç®—æœ€ç»ˆåˆ†æ•°
        final_all_cnt = 0
        final_commonsense_cnt = 0
        final_hardConstraint_cnt = 0
        
        constraint_dis_record = {"commonsense":{"pass":0,"total":0},"hard":{"pass":0,"total":0}}
        
        # è¯¦ç»†çš„çº¦æŸç»Ÿè®¡å¤„ç†ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        for constraint in ['commonsense','hard']:
            if constraint == 'commonsense':
                constraint_statistic = commonsenseConstraint_statistic_processed
                key_list = ['is_valid_information_in_current_city','is_valid_information_in_sandbox','is_reasonalbe_visiting_city','is_valid_restaurants','is_valid_transportation','is_valid_attractions','is_valid_accommodation','is_not_absent']
            else:
                constraint_statistic = hardConstraint_statistic_processed
                key_list = ['valid_cost','valid_room_rule','valid_cuisine','valid_room_type','valid_transportation']
            
            for level in constraint_statistic:
                for day in constraint_statistic[level]:
                    for key in key_list:
                        if key in constraint_statistic[level][day]:
                            constraint_dis_record[constraint]['pass'] += constraint_statistic[level][day][key]['true']
                            if constraint == 'commonsense':
                                constraint_dis_record[constraint]['total'] += count_record[level][day]
                            else:
                                if key in ['valid_room_rule','valid_cuisine','valid_room_type','valid_transportation']:
                                    # åªæœ‰ medium å’Œ hard çº§åˆ«æœ‰ mapping_constraint_record
                                    if level in ['medium', 'hard']:
                                        constraint_dis_record[constraint]['total'] += mapping_constraint_record[level][day].get(key, 0)
                                    # easy çº§åˆ«ç›´æ¥ä½¿ç”¨ count_record
                                    else:
                                        constraint_dis_record[constraint]['total'] += count_record[level][day]
                                else:
                                    constraint_dis_record[constraint]['total'] += count_record[level][day]
        
        # è®¡ç®—å®è§‚é€šè¿‡ç‡
        for idx in range(0, min(len(self.query_data_list), len(plan_constraint_store))):
            if plan_constraint_store[idx]['commonsense_constraint']:
                final_commonsense_pass = True
                final_hardConstraint_pass = True
                
                # æ£€æŸ¥å¸¸è¯†çº¦æŸ
                for item in plan_constraint_store[idx]['commonsense_constraint']:
                    if plan_constraint_store[idx]['commonsense_constraint'][item][0] is not None and not plan_constraint_store[idx]['commonsense_constraint'][item][0]:
                        final_commonsense_pass = False
                        break
                
                # æ£€æŸ¥ç¡¬çº¦æŸ
                if plan_constraint_store[idx]['hard_constraint'] is None:
                    continue
                    
                for item in plan_constraint_store[idx]['hard_constraint']:
                    if plan_constraint_store[idx]['hard_constraint'][item][0] is not None and plan_constraint_store[idx]['hard_constraint'][item][0] == False:
                        final_hardConstraint_pass = False
                        break
                
                if final_commonsense_pass:
                    final_commonsense_cnt += 1
                if final_hardConstraint_pass:
                    final_hardConstraint_cnt += 1
                if final_commonsense_pass and final_hardConstraint_pass:
                    final_all_cnt += 1
        
        # è®¡ç®—æœ€ç»ˆç»“æœ
        result = {}
        
        # æ ¹æ®æ•°æ®é›†ç±»å‹è®¾ç½®æ€»æ•°
        if set_type == 'train':
            total_count = 45
            commonsense_total = 360
            hard_total = 105
        elif set_type == 'validation':
            total_count = 180
            commonsense_total = 1440
            hard_total = 420
        elif set_type == 'test':
            total_count = 1000
            commonsense_total = 8000
            hard_total = 2290
        else:
            # åŠ¨æ€è®¡ç®—
            total_count = min(len(self.query_data_list), len(tested_plans))
            commonsense_total = constraint_dis_record['commonsense']['total'] or total_count * 8
            hard_total = constraint_dis_record['hard']['total'] or 1
        
        result['Delivery Rate'] = delivery_cnt / total_count if total_count > 0 else 0
        result['Commonsense Constraint Micro Pass Rate'] = constraint_dis_record['commonsense']['pass'] / commonsense_total if commonsense_total > 0 else 0
        result['Commonsense Constraint Macro Pass Rate'] = final_commonsense_cnt / total_count if total_count > 0 else 0
        result['Hard Constraint Micro Pass Rate'] = constraint_dis_record['hard']['pass'] / hard_total if hard_total > 0 else 0
        result['Hard Constraint Macro Pass Rate'] = final_hardConstraint_cnt / total_count if total_count > 0 else 0
        result['Final Pass Rate'] = final_all_cnt / total_count if total_count > 0 else 0
        
        # è¯¦ç»†ç»“æœ
        remap_commonsense_constraint_record, remap_hard_constraint_record = self.paper_term_mapping(
            commonsenseConstraint_statistic_processed, hardConstraint_statistic_processed)
        
        detailed_scores = {
            "Commonsense Constraint": remap_commonsense_constraint_record,
            "Hard Constraint": remap_hard_constraint_record
        }
        
        return result, detailed_scores
    
    def statistics(self, constraint_statistic):
        """ç»Ÿè®¡çº¦æŸç»“æœ"""
        result = {level: {day: {} for day in constraint_statistic[level]} for level in constraint_statistic}
        
        for level, days in constraint_statistic.items():
            for day, dicts in days.items():
                for dct in dicts:
                    if dct:
                        for key, data in dct.items():
                            true_count = data.count(True) if isinstance(data, list) else (1 if data[0] else 0)
                            false_count = data.count(False) if isinstance(data, list) else (0 if data[0] else 1)
                            if key not in result[level][day]:
                                result[level][day][key] = {"true": 0, "false": 0}
                            result[level][day][key]["true"] += true_count
                            result[level][day][key]["false"] += false_count
        
        return result
    
    def paper_term_mapping(self, commonsense_constraint_record, hard_constraint_record):
        """æ˜ å°„è®ºæ–‡æœ¯è¯­"""
        mapping_dict = {
            'is_valid_information_in_current_city': 'Within Current City',
            'is_valid_information_in_sandbox': 'Within Sandbox',
            'is_reasonalbe_visiting_city': 'Reasonable City Route',
            'is_valid_restaurants': 'Diverse Restaurants',
            'is_valid_transportation': 'Non-conf. Transportation',
            'is_valid_attractions': 'Diverse Attractions',
            'is_valid_accommodation': 'Minimum Nights Stay',
            'is_not_absent': 'Complete Information',
            'valid_cost': 'Budget',
            'valid_room_rule': 'Room Rule',
            'valid_cuisine': 'Cuisine',
            'valid_room_type': 'Room Type',
            'valid_transportation': 'Transportation'
        }
        
        remap_commonsense = {level: {day: {} for day in [3,5,7]} for level in ['easy','medium','hard']}
        remap_hard = {level: {day: {} for day in [3,5,7]} for level in ['easy','medium','hard']}
        
        for level in commonsense_constraint_record:
            for day in commonsense_constraint_record[level]:
                remap_commonsense[level][day] = {
                    mapping_dict.get(key, key): val 
                    for key, val in commonsense_constraint_record[level][day].items()
                }
                remap_hard[level][day] = {
                    mapping_dict.get(key, key): val 
                    for key, val in hard_constraint_record[level][day].items()
                }
        
        return remap_commonsense, remap_hard
    
    def load_line_json_data(self, filename):
        """åŠ è½½ JSONL æ–‡ä»¶"""
        data = []
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f.read().strip().split('\n'):
                if line.strip():
                    unit = json.loads(line)
                    data.append(unit)
        return data
    
    def save_results(self, scores, detailed_scores):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        print("\nğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ...")
        
        # ä»è¾“å…¥æ–‡ä»¶åæ´¾ç”Ÿç»“æœæ–‡ä»¶å
        base_name = os.path.basename(self.args.input_file)
        if base_name.endswith('.json'):
            base_name = base_name[:-5]
        
        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æ ¼å¼åŒ–åˆ†æ•°ç»“æœä¸ºç™¾åˆ†æ¯”
        formatted_scores = {}
        for key in scores:
            formatted_scores[key] = f"{float(int(scores[key]*10000))/100}%"
        
        # ä¿å­˜åˆå¹¶ç»“æœ
        combined_results = {
            'scores': formatted_scores,
            'detailed_scores': detailed_scores,
            'evaluation_file': self.submission_file,
            'source_file': self.args.input_file,
            'timestamp': timestamp
        }
        
        combined_filename = f"{base_name}_results.json"
        combined_path = os.path.join(self.results_dir, combined_filename)
        
        with open(combined_path, 'w', encoding='utf-8') as f:
            json.dump(combined_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {combined_path}")
        return combined_path
    
    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹"""
        print("ğŸš€ å¼€å§‹é›†æˆè¯„ä¼°æµç¨‹...")
        print("="*60)
        
        start_time = time.time()
        
        try:
            # æ­¥éª¤1: è§£æ
            if not self.step1_parsing():
                print("âŒ æ­¥éª¤1å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ")
                return False
            
            # æ­¥éª¤2: æå–
            if not self.step2_element_extraction():
                print("âŒ æ­¥éª¤2å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ")
                return False
            
            # æ­¥éª¤3: åˆå¹¶
            if not self.step3_combination():
                print("âŒ æ­¥éª¤3å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ")
                return False
            
            # æ­¥éª¤4: è¯„ä¼°
            success, _, _ = self.step4_evaluation()
            if not success:
                print("âŒ æ­¥éª¤4å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ")
                return False
            
            end_time = time.time()
            total_time = end_time - start_time
            
            print("\n" + "="*60)
            print("âœ… é›†æˆè¯„ä¼°å®Œæˆ!")
            print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’")
            print("="*60)
            
            return True
            
        except Exception as e:
            print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            traceback.print_exc()
            return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="é›†æˆè¯„ä¼°è„šæœ¬")
    parser.add_argument("input_file", type=str,
                       help="åŒ…å«æ¨¡å‹ç”Ÿæˆå†…å®¹çš„è¾“å…¥JSONæ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚: output/train_v0_20250819203412.json)")
    
    args = parser.parse_args()
    
    print("ğŸ“‹ è¯„ä¼°é…ç½®:")
    print(f"  è¾“å…¥æ–‡ä»¶: {args.input_file}")
    print()
    
    # åˆ›å»ºè¯„ä¼°å™¨å¹¶è¿è¡Œ
    evaluator = IntegratedEvaluator(args)
    success = evaluator.run()
    
    if success:
        print("ğŸ‰ è¯„ä¼°æˆåŠŸå®Œæˆ!")
        return 0
    else:
        print("ğŸ’¥ è¯„ä¼°å¤±è´¥!")
        return 1


if __name__ == "__main__":
    exit(main())
